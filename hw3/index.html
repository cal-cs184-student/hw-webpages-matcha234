<html>
	<head>
		<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default'></script>
		<link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600&display=swap" rel="stylesheet">
		<style>
			h1 {
				text-align: center;
			}

			.container {
				margin: 0 auto;
				padding: 60px 20%;
			}

			figure {
				text-align: center;
			}

			img {
				display: inline-block;
			}

			body {
				font-family: 'Inter', sans-serif;
			}
		</style>
	</head>
	<body>
		<div class="container">
		<h1>CS184/284A Spring 2025 Homework 3 Write-Up</h1>
		<div style="text-align: center;">Advika Bhike and Malavikha Sudarshan</div>

		<br>

		Link to webpage: <a href="https://cal-cs184-student.github.io/hw-webpages-matcha234/hw3/index.html">cal-cs184-student.github.io/hw-webpages-matcha234/hw3/index.html</a>
		<br>

		Link to GitHub repository: <a href="https://github.com/cal-cs184-student/sp25-hw3-matchaaa">github.com/cal-cs184-student/sp25-hw3-matchaaa</a>
		
		<figure>
			<img src="cornell.png" alt="Cornell Boxes with Bunnies" style="width:70%"/>
		</figure>

		<!--
		We've already added one heading per part, to make your write-up as navigable when grading. Please fit your write-up within these sections!
		-->

		<h2>Overview</h2>
		Give a high-level overview of what you implemented in this homework. Think about what you've built as a whole. Share your thoughts on what interesting things you've learned from completing the homework.

		<h2>Part 1: Ray Generation and Scene Intersection</h2>

		<h3>Ray Generation and Primitive Intersections</h3>
		For ray generation, we first converted the given normalised image coordinates (x, y) into camera space. The virtual sensor was defined on the z = -1 plane, and we mapped 
		(x, y) to a point on the sensor using the field of view (FOV). The bottom-left and top-right 
		corners of the sensor were computed using:</p>
		
		<pre>
		left = -tan(hFov / 2), right = tan(hFov / 2)
		bottom = -tan(vFov / 2), top = tan(vFov / 2)
		</pre>
		
		<p>Using these values, we calculated the corresponding point on the sensor as:</p>
		
		<pre>
		sensor_x = left + (right - left) * x
		sensor_y = bottom + (top - bottom) * y
		</pre>
		<p>We then created a ray starting from the camera origin (0,0,0) and passing through this 
		calculated sensor point. The direction was initially set in camera space, then transformed 
		to world space using the camera-to-world transformation matrix <code>c2w</code> provided. Finally, we normalised 
		the direction vector and set the ray's near and far clipping distances to <code>nclip</code> and <code>fclip</code>.</p>
		
		<p>For triangle-ray intersections, we implemented a variation of the Möller-Trumbore algorithm. We first defined the edges of the triangle:</p>
		
		<pre>
		edge1 = v1 - v0
		edge2 = v2 - v0
		</pre>
		
		<p>Then, we calculated the determinant using the cross product:</p>
		
		<pre>
		h = cross(ray.dir, edge2)
		det = dot(edge1, h)
		</pre>
		
		<p>If the determinant was close to zero, the ray was parallel to the triangle, and so we ignored it.</p>
		
		<p>We then calculated barycentric coordinates to determine if the intersection point was inside the triangle:</p>
		
		<pre>
		f = 1.0 / det
		s = ray.origin - v0
		u = f * dot(s, h)
		</pre>
		
		<p>If u was not between 0 and 1, we discarded the intersection. We did something similar for v:</p>
		
		<pre>
		q = cross(s, edge1)
		v = f * dot(ray.dir, q)
		</pre>
		
		<p>If v was out of range or if u + v exceeded 1, then the intersection was outside the triangle.</p>
		
		<p>Finally, we computed t (the intersection distance along the ray):</p>
		
		<pre>
		t = f * dot(edge2, q)
		</pre>
		
		<p>If t was within the valid range (between <code>min_t</code> and <code>max_t</code> of the ray), we updated <code>max_t</code> to 
		this new intersection distance and stored the intersection details, including the interpolated normal 
		using barycentric coordinates.</p>

		<h3>Triangle Intersection</h3>
		<p>We first implemented <code>has_intersection</code>, a function that checked if a ray intersected the triangle without actually recording the details of the intersection. To implement this function, we did the following:</p>
		<ul>
			<li>First, we got the ray’s origin and direction.</li>
			<li>Then, we defined the triangle edges and calculated the normal vector using the cross product.</li>
			<li>We computed the intersection of the ray with the triangle’s plane and checked if it was within valid bounds.</li>
			<li>Using barycentric coordinates, we verified if the intersection point lay inside the triangle. If it did, we returned <code>true</code> and updated the ray’s <code>max_t</code>.</li>
		</ul>
	
		<p>The <code>intersect</code> function was similar to <code>has_intersection</code>, but it also recorded the intersection data when an intersection happened.</p>
		<ul>
			<li>First, we called <code>has_intersection</code> to check if the ray intersected the triangle at all. If not, we could immediately return <code>false</code>.</li>
			<li>We set up the ray’s origin, direction, and triangle’s edges again.</li>
			<li>Using the <code>max_t</code> from <code>has_intersection</code>, we computed the exact intersection point.</li>
			<li>We used barycentric coordinates to calculate the normal at the intersection point.</li>
			<li>Then, we stored the intersection details, including the intersection distance, normal, and material properties.</li>
		</ul>
		
		<p>Here are some images with normal shading:</p>
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
				  <img src="task1/CBempty.png" width="300px"/>
				  <figcaption>Cornell Box empty room</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="task1/CBspheres.png" width="300px"/>
				  <figcaption>Cornell Box spheres</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="task1/CBgems.png" width="300px"/>
				  <figcaption>Cornell Box gems</figcaption>
				</td>
			</tr>
			</table>
		</div>
		
		<h2>Part 2: Bounding Volume Hierarchy</h2>
		<p>For Part 2 of this homework, we implemented a BVH (Bounding Volume Hierarchy) construction algorithm to optimise ray-triangle intersection queries in our renderer, using a median primitive heuristic.</p>
	
		<p>We began by calculating the bounding box for the entire set of primitives within the range <code>start</code> to <code>end</code>. 
		We iterated through the primitives, expanding the bounding box <code>bbox</code> to surround all of them. 
		This bounding box represents the enclosing volume for the new BVH node. 
		
		<p>Next, we checked if the number of primitives in the newly created node was less than or equal to <code>max_leaf_size</code> to determine whether that node was a leaf node. 
			If it was a leaf node, we set the <code>start</code> and <code>end</code> iterators for the node and returned the node without instantiating the left and right pointers.
			If the node contained more than <code>max_leaf_size</code> primitives, we split the node into two child nodes, using the median primitive heuristic.
			To calculate this heuristic, we found the longest axis (between x, y or z) and sorted all the primitives based on their centroid position along that longest axis. 
			We calculated the midpoint of start and end to divide the now sorted vector of primitives, and recursively constructed our left and right nodes with different halves of the vector.
		</p>

		<p>We decided to split the primitives along the longest axis according to the median primitive because it helps create a fairer distribution between the two child nodes. 
			This makes it easier to calculate ray-primitive intersections because there's a more even distribution between nodes (as opposed to having less even-splitting heuristics, which could require
			something like searching an entire child node that has all primitives from the overall bounding box except one). 
			Essentially, splitting along the largest axis using the median primitive allows for better separation, which improves efficiency when checking for intersections.
		</p>

		<p>Here are some large .dae files with normal shading that can only be rendered feasibly with BVH acceleration:</p>
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
					<img src="task2/CBBunny.png" width="300px"/>
					<figcaption>Cornell Box bunny</figcaption>
				  </td>
				<td style="text-align: center;">
				  <img src="task2/CBdragon.png" width="300px"/>
				  <figcaption>Cornell Box dragon</figcaption>
				</td>
			</tr>
			<tr>
				<td style="text-align: center;">
				  <img src="task2/maxplanck.png" width="300px"/>
				  <figcaption>Max Planck</figcaption>
				</td>
				<td style="text-align: center;">
					<img src="task2/CBlucy.png" width="300px"/>
					<figcaption>Cornell Box Lucy</figcaption>
				  </td>
			</tr>
			</table>
		</div>

		<p>
			We compared the rendering times (BVH vs. no BVH) for a few images.
			The Cornell Box Bunny image renders in 0.0349s seconds with the BVH implementation, and in 66.7730s without.
			The Max Planck image took 0.0444s seconds with our BVH implementation and 98.1858s seconds without. 
			Cornell Box Lucy took 0.0451s seconds with our BVH implementation and 321.5604s seconds without.
			These are images with some highly complex geometries, and our BVH implementation helps render images that normally take multiple minutes in less than a second. 
		</p>
		
		<h2>Part 3: Direct Illumination</h2>

		<p>
			In this part, we implemented two type of direct lighting: one that uses uniform hemisphere sampling and another that uses importance sampling.
		</p>

		<p>
			First, we sample our light direction and convert this direction into world coordinates. 
			We then trace a ray from the point hit_p (point of interest derived from the original ray of interest and original intersection of interest) in the direction of the sampled light source and check whether this new ray will intersect with a light source. 
			If it does intersect a light source, we use the following reflection equation to calculate the outgoing light.
		</p>

		<td style="text-align: center;">
			<img src="task3/reflection_equation.png" width="300px"/>
			<figcaption>Reflection equation from lecture</figcaption>
		  </td>
		
		<p>
			For each sampled light direction, we compute the inner term and keep summing these terms for num_samples number of times (this is variable and is set when we run commands to render images).
			After summing, we divide by num_samples to take the average of this sum.
		</p>

		<p>
			Importance sampling, on the other hand, samples all lights directly rather than uniformly on a hemisphere. 
			This type of sampling also uses a Monte Carlo estimator. 
		</p>
		<p>
			We iterate through the light sources in the scene and sample them num_samples number of times (which is variable and set when the rendering command is run). 
			If the light source is a point light source, we only sample it once though. 
			Otherwise, for each sample in num_samples, we sample directions from the light source and hit_p and cast a ray in this direction. 
			If this new ray does not intersect any object, then we know it’s a valid light-casting ray, and so we calculate the reflectance equation for it. 
			We take the Monte Carlo estimate of each light source and then add it to our global sum <code>L_out</code>. 
			It’s important to note that we need to take the average for each light source before adding it to the global sum because sometimes num_samples can be 1 (e.g. if the light is a point light) and other times it can be another number. 
		</p>

		<p>Here are some images rendered by both types of direct lighting. The sample rate is varied to show how both sampling techniques progress with different sampling rates</p>
		<!-- insert photo -->

		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
					<img src="task3/CBbunny_H_16_8.png" width="300px"/>
					<figcaption>Cornell Box bunny with 16 samples/pixel and 8 samples/area_light using uniform hemisphere sampling</figcaption>
				  </td>
				<td style="text-align: center;">
				  <img src="task3/CBbunny_16_8.png" width="300px"/>
				  <figcaption>Cornell Box bunny with 16 samples/pixel and 8 samples/area_light using importance sampling</figcaption>
				</td>
			</tr>
			<tr>
				<td style="text-align: center;">
				  <img src="task3/spheres_H_64_32.png" width="300px"/>
				  <figcaption>Cornell Box spheres with 64 samples/pixel and 32 samples/area_light using uniform hemisphere sampling</figcaption>
				</td>
				<td style="text-align: center;">
					<img src="task3/spheres_64_32.png" width="300px"/>
					<figcaption>Cornell Box spheres with 64 samples/pixel and 32 samples/area_light using importance sampling</figcaption>
				  </td>
			</tr>
			</table>
		</div>

		<p>Here's how the Cornell Box bunny image varies depending on light sample rate for direct light sampling. 
			The number of camera rays per pixels is held at a constant rate 1.</p>
		<!-- insert photo -->

		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			<tr>
				<td style="text-align: center;">
					<img src="task3/vary_sample_rate/CBBunny_1.png" width="300px"/>
					<figcaption>Light Sample Rate 1</figcaption>
				  </td>
			</tr>
			<tr>
				<td style="text-align: center;">
				  <img src="task3/vary_sample_rate/CBBunny_4.png" width="300px"/>
				  <figcaption>Light Sample Rate 4</figcaption>
				</td>
			</tr>
			<tr>
				<td style="text-align: center;">
				  <img src="task3/vary_sample_rate/CBBunny_16.png" width="300px"/>
				  <figcaption>Light Sample Rate 16</figcaption>
				</td>
			</tr>
			<tr>
				<td style="text-align: center;">
					<img src="task3/vary_sample_rate/CBBunny_64.png" width="300px"/>
					<figcaption>Light Sample Rate 64</figcaption>
				  </td>
			</tr>
			</table>
		</div>

		<p>
			The noise in the shadows decreases and becomes more blurred, more realistic and less disconnected as the light sample rate increases.
		</p>

		<p>
			In general, we can notice that the images using importance sampling tend to be less noisy. 
			This is because they directly sample from the light source, rather than assume the light source follows a uniform distribution. 
			Additionally, as both the camera sample rate and light sample rate increase, images rendered with importance sampling tend to converge faster to a high quality, less-noisy image than uniform sampling does as demonstrated in the images below. 
			We can also see a clear progression of difference when only the light sample rate is changed when using importance light sampling. 
		</p>


		<h2>Part 4: Global Illumination</h2>
		Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.

		<h2>Part 5: Adaptive Sampling</h2>
		Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.

		<h2>(Optional) Part 6: Extra Credit Opportunities</h2>
		Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.
		
		<h2>Additional Notes (please remove)</h2>
		<ul>
			<li>You can also add code if you'd like as so: <code>code code code</code></li>
			<li>If you'd like to add math equations, 
				<ul>
					<li>You can write inline equations like so: \( a^2 + b^2 = c^2 \)</li>
					<li>You can write display equations like so: \[ a^2 + b^2 = c^2 \]</li>
				</ul>
			</li>
		</ul>
		</div>
	</body>
</html>