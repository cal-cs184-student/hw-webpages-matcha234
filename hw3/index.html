<html>
	<head>
		<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default'></script>
		<link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600&display=swap" rel="stylesheet">
		<style>
			h1 {
				text-align: center;
			}

			.container {
				margin: 0 auto;
				padding: 60px 20%;
			}

			figure {
				text-align: center;
			}

			img {
				display: inline-block;
			}

			body {
				font-family: 'Inter', sans-serif;
			}
		</style>
	</head>
	<body>
		<div class="container">
		<h1>CS184/284A Spring 2025 Homework 3 Write-Up</h1>
		<div style="text-align: center;">Advika Bhike and Malavikha Sudarshan</div>

		<br>

		Link to webpage: <a href="https://cal-cs184-student.github.io/hw-webpages-matcha234/hw3/index.html">cal-cs184-student.github.io/hw-webpages-matcha234/hw3/index.html</a>
		<br>

		Link to GitHub repository: <a href="https://github.com/cal-cs184-student/sp25-hw3-matchaaa">github.com/cal-cs184-student/sp25-hw3-matchaaa</a>
		
		<figure>
			<img src="cornell.png" alt="Cornell Boxes with Bunnies" style="width:70%"/>
		</figure>

		<!--
		We've already added one heading per part, to make your write-up as navigable when grading. Please fit your write-up within these sections!
		-->

		<h2>Overview</h2>
		Give a high-level overview of what you implemented in this homework. Think about what you've built as a whole. Share your thoughts on what interesting things you've learned from completing the homework.

		<h2>Part 1: Ray Generation and Scene Intersection</h2>

		<h3>Ray Generation and Primitive Intersections</h3>
		For ray generation, we first converted the given normalised image coordinates (x, y) into camera space. The virtual sensor was defined on the z = -1 plane, and we mapped 
		(x, y) to a point on the sensor using the field of view (FOV). The bottom-left and top-right 
		corners of the sensor were computed using:</p>
		
		<pre>
		left = -tan(hFov / 2), right = tan(hFov / 2)
		bottom = -tan(vFov / 2), top = tan(vFov / 2)
		</pre>
		
		<p>Using these values, we calculated the corresponding point on the sensor as:</p>
		
		<pre>
		sensor_x = left + (right - left) * x
		sensor_y = bottom + (top - bottom) * y
		</pre>
		<p>We then created a ray starting from the camera origin (0,0,0) and passing through this 
		calculated sensor point. The direction was initially set in camera space, then transformed 
		to world space using the camera-to-world transformation matrix <code>c2w</code> provided. Finally, we normalised 
		the direction vector and set the ray's near and far clipping distances to <code>nclip</code> and <code>fclip</code>.</p>
		
		<p>For triangle-ray intersections, we implemented a variation of the Möller-Trumbore algorithm. We first defined the edges of the triangle:</p>
		
		<pre>
		edge1 = v1 - v0
		edge2 = v2 - v0
		</pre>
		
		<p>Then, we calculated the determinant using the cross product:</p>
		
		<pre>
		h = cross(ray.dir, edge2)
		det = dot(edge1, h)
		</pre>
		
		<p>If the determinant was close to zero, the ray was parallel to the triangle, and so we ignored it.</p>
		
		<p>We then calculated barycentric coordinates to determine if the intersection point was inside the triangle:</p>
		
		<pre>
		f = 1.0 / det
		s = ray.origin - v0
		u = f * dot(s, h)
		</pre>
		
		<p>If u was not between 0 and 1, we discarded the intersection. We did something similar for v:</p>
		
		<pre>
		q = cross(s, edge1)
		v = f * dot(ray.dir, q)
		</pre>
		
		<p>If v was out of range or if u + v exceeded 1, then the intersection was outside the triangle.</p>
		
		<p>Finally, we computed t (the intersection distance along the ray):</p>
		
		<pre>
		t = f * dot(edge2, q)
		</pre>
		
		<p>If t was within the valid range (between <code>min_t</code> and <code>max_t</code> of the ray), we updated <code>max_t</code> to 
		this new intersection distance and stored the intersection details, including the interpolated normal 
		using barycentric coordinates.</p>

		<h3>Triangle Intersection</h3>
		<p>We first implemented <code>has_intersection</code>, a function that checked if a ray intersected the triangle without actually recording the details of the intersection. To implement this function, we did the following:</p>
		<ul>
			<li>First, we got the ray’s origin and direction.</li>
			<li>Then, we defined the triangle edges and calculated the normal vector using the cross product.</li>
			<li>We computed the intersection of the ray with the triangle’s plane and checked if it was within valid bounds.</li>
			<li>Using barycentric coordinates, we verified if the intersection point lay inside the triangle. If it did, we returned <code>true</code> and updated the ray’s <code>max_t</code>.</li>
		</ul>
	
		<p>The <code>intersect</code> function was similar to <code>has_intersection</code>, but it also recorded the intersection data when an intersection happened.</p>
		<ul>
			<li>First, we called <code>has_intersection</code> to check if the ray intersected the triangle at all. If not, we could immediately return <code>false</code>.</li>
			<li>We set up the ray’s origin, direction, and triangle’s edges again.</li>
			<li>Using the <code>max_t</code> from <code>has_intersection</code>, we computed the exact intersection point.</li>
			<li>We used barycentric coordinates to calculate the normal at the intersection point.</li>
			<li>Then, we stored the intersection details, including the intersection distance, normal, and material properties.</li>
		</ul>
		
		<p>Here are some images with normal shading:</p>
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
				  <img src="task1/CBempty.png" width="300px"/>
				  <figcaption>Cornell Box empty room</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="task1/CBspheres.png" width="300px"/>
				  <figcaption>Cornell Box spheres</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="task1/CBgems.png" width="300px"/>
				  <figcaption>Cornell Box gems</figcaption>
				</td>
			</tr>
			</table>
		</div>
		
		<h2>Part 2: Bounding Volume Hierarchy</h2>
		<p>For Part 2 of this homework, we implemented a BVH (Bounding Volume Hierarchy) construction algorithm to optimise ray-triangle intersection queries in our renderer, using a median primitive heuristic.</p>
	
		<p>We began by calculating the bounding box for the entire set of primitives within the range <code>start</code> to <code>end</code>. 
		We iterated through the primitives, expanding the bounding box <code>bbox</code> to surround all of them. 
		This bounding box represents the enclosing volume for the new BVH node. 
		
		<p>Next, we checked if the number of primitives in the newly created node was less than or equal to <code>max_leaf_size</code> to determine whether that node was a leaf node. 
			If it was a leaf node, we set the <code>start</code> and <code>end</code> iterators for the node and returned the node without instantiating the left and right pointers.
			If the node contained more than <code>max_leaf_size</code> primitives, we split the node into two child nodes, using the median primitive heuristic.
			To calculate this heuristic, we found the longest axis (between x, y or z) and sorted all the primitives based on their centroid position along that longest axis. 
			We calculated the midpoint of start and end to divide the now sorted vector of primitives, and recursively constructed our left and right nodes with different halves of the vector.
		</p>

		<p>We decided to split the primitives along the longest axis according to the median primitive because it helps create a fairer distribution between the two child nodes. 
			This makes it easier to calculate ray-primitive intersections because there's a more even distribution between nodes (as opposed to having less even-splitting heuristics, which could require
			something like searching an entire child node that has all primitives from the overall bounding box except one). 
			Essentially, splitting along the largest axis using the median primitive allows for better separation, which improves efficiency when checking for intersections.
		</p>

		<p>Here are some large .dae files with normal shading that can only be rendered feasibly with BVH acceleration:</p>
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
					<img src="task2/CBBunny.png" width="300px"/>
					<figcaption>Cornell Box bunny</figcaption>
				  </td>
				<td style="text-align: center;">
				  <img src="task2/CBdragon.png" width="300px"/>
				  <figcaption>Cornell Box dragon</figcaption>
				</td>
			</tr>
			<tr>
				<td style="text-align: center;">
				  <img src="task2/maxplanck.png" width="300px"/>
				  <figcaption>Max Planck</figcaption>
				</td>
				<td style="text-align: center;">
					<img src="task2/CBlucy.png" width="300px"/>
					<figcaption>Cornell Box Lucy</figcaption>
				  </td>
			</tr>
			</table>
		</div>

		<p>
			We compared the rendering times (BVH vs. no BVH) for a few images.
			The Cornell Box Bunny image renders in 0.0349s seconds with the BVH implementation, and in 66.7730s without.
			The Max Planck image took 0.0444s seconds with our BVH implementation and 98.1858s seconds without. 
			Cornell Box Lucy took 0.0451s seconds with our BVH implementation and 321.5604s seconds without.
			These are images with some highly complex geometries, and our BVH implementation helps render images that normally take multiple minutes in less than a second. 
		</p>
		
		<h2>Part 3: Direct Illumination</h2>

		<p>
			In this part, we implemented two types of direct lighting: one that uses uniform hemisphere sampling and another that uses importance sampling.
		</p>

		<p>
			First, we sampled our light direction and converted this direction into world coordinates. 
			We then traced a ray from the point <code>hit_p</code> (point of interest derived from the original ray of interest and original intersection of interest) in the direction of the sampled light source and checked whether this new ray would intersect with a light source. 
			If it did intersect a light source, we used the following reflection equation to calculate the outgoing light.
		</p>

		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
				<td style="text-align: center;">
					<img src="task3/reflection_equation.png" width="300px"/>
					<figcaption>Reflectance equation from lecture</figcaption>
				</td>
			</table>
		</div>
		
		<p>
			For each sampled light direction, we computed the inner term and kept summing these terms for num_samples number of times (this was variable and was set when we ran commands to render images).
			After summing, we divided by num_samples to take the average of this sum.
		</p>

		<p>
			Importance sampling, on the other hand, sampled all lights directly rather than uniformly on a hemisphere. 
			This type of sampling also used a Monte Carlo estimator. 
		</p>
		<p>
			We iterated through the light sources in the scene and sampled them num_samples number of times (which was variable and set when the rendering command was run). 
			If the light source was a point light source, we only sampled it once. 
			Otherwise, for each sample in num_samples, we sampled directions between the light source and <code>hit_p</code> and cast a ray in this direction. 
			If this new ray did not intersect any object, then we knew that it was a valid light-casting ray, and so we calculated the reflectance equation for it. 
			We took the Monte Carlo estimate of each light source and then added it to our global sum <code>L_out</code>. 
			It was important to note that we needed to compute a seperate Monte Carlo estimator for each light source 
			(specifically, take the average for each specific light source before adding to <code>L_out</code>) before adding it to the global sum because sometimes num_samples could be 1 (e.g. if the light was a point light) and other times it could be the given sample_rate. 
		</p>

		<p>Here are some images rendered by both types of direct lighting. The sample rate was varied to show how both sampling techniques progress with different sampling rates.</p>
		<!-- insert photo -->

		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
					<img src="task3/uniform/CBbunny_H_16_8.png" width="300px"/>
					<figcaption>Cornell Box bunny with 16 samples/pixel and 8 samples/area_light using uniform hemisphere sampling</figcaption>
				  </td>
				<td style="text-align: center;">
				  <img src="task3/importance/CBbunny_16_8.png" width="300px"/>
				  <figcaption>Cornell Box bunny with 16 samples/pixel and 8 samples/area_light using importance sampling</figcaption>
				</td>
			</tr>
			<tr>
				<td style="text-align: center;">
				  <img src="task3/uniform/CBbunny_H_64_32.png" width="300px"/>
				  <figcaption>Cornell Box bunny with 64 samples/pixel and 32 samples/area_light using uniform hemisphere sampling</figcaption>
				</td>
				<td style="text-align: center;">
					<img src="task3/importance/bunny_64_32.png" width="300px"/>
					<figcaption>Cornell Box bunny with 64 samples/pixel and 32 samples/area_light using importance sampling</figcaption>
				  </td>
			</tr>

			<tr>
				<td style="text-align: center;">
				  <img src="task3/uniform/spheres_H_64_32.png" width="300px"/>
				  <figcaption>Cornell Box spheres with 64 samples/pixel and 32 samples/area_light using uniform hemisphere sampling</figcaption>
				</td>
				<td style="text-align: center;">
					<img src="task3/importance/spheres_64_32.png" width="300px"/>
					<figcaption>Cornell Box spheres with 64 samples/pixel and 32 samples/area_light using importance sampling</figcaption>
				  </td>
			</tr>
			</table>
		</div>

		<p>Here's how the Cornell Box bunny image varies for different light sample rates for direct light sampling. 
			The number of camera rays per pixels was held at a constant rate 1.</p>
		<!-- insert photo -->

		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			<tr>
				<td style="text-align: center;">
					<img src="task3/vary_sample_rate/CBBunny_1.png" width="300px"/>
					<figcaption>Light Sample Rate 1</figcaption>
				  </td>
			</tr>
			<tr>
				<td style="text-align: center;">
				  <img src="task3/vary_sample_rate/CBBunny_4.png" width="300px"/>
				  <figcaption>Light Sample Rate 4</figcaption>
				</td>
			</tr>
			<tr>
				<td style="text-align: center;">
				  <img src="task3/vary_sample_rate/CBBunny_16.png" width="300px"/>
				  <figcaption>Light Sample Rate 16</figcaption>
				</td>
			</tr>
			<tr>
				<td style="text-align: center;">
					<img src="task3/vary_sample_rate/CBBunny_64.png" width="300px"/>
					<figcaption>Light Sample Rate 64</figcaption>
				  </td>
			</tr>
			</table>
		</div>

		<p>
			The noise in the shadows decreased and became more blurred, more realistic and less disconnected as the light sample rate increased.
		</p>

		<p>
			In general, we can notice that the images using importance sampling tend to be less noisy. 
			This is because they directly sample from the light source, rather than assume the light source follows a uniform distribution. 
			Additionally, as both the camera sample rate and light sample rate increase, images rendered with importance sampling tend to converge faster to a higher quality, less-noisy image than uniform sampling does as demonstrated in the images above. 
			We can also see a clear progression of difference when only the light sample rate is changed when using importance light sampling. 
		</p>


		<h2>Part 4: Global Illumination</h2>

		<p>First, we built a local coordinate system based on the surface normal at the intersection point. We used <code>make_coord_space(o2w, isect.n)</code> to create an orthonormal basis and then took its transpose <code>w2o</code> to convert between world and local space. We calculated the intersection point <code>hit_p</code> and the outgoing direction <code>w_out</code> in local space using <code>w2o * (-r.d)</code>.</p>
        <p>We initialized <code>L_out</code> to black (0,0,0) and added direct lighting using <code>one_bounce_radiance</code>, but only if <code>isAccumBounces</code> was true. If we were only interested in indirect lighting, we skipped this step.</p>
        <p>Next, we used <code>isect.bsdf->sample_f(w_out, &w_i, &pdf)</code> to sample a new bounce direction, getting a random <code>w_i</code> along with its probability density function (pdf). If <code>pdf</code> was too small, we returned out of the function. We converted <code>w_i</code> from local to world space using <code>w_i_world = o2w * w_i</code> and created a new ray <code>new_ray</code> starting at <code>hit_p</code>. Then, we checked if the ray hit something using <code>bvh->intersect(new_ray, &new_isect)</code>. If there was an intersection, we recursively called <code>at_least_one_bounce_radiance</code> and scaled it by the BSDF, cosine term, and pdf.</p>
 
        <p>To avoid infinite recursion, we used Russian Roulette Termination with a 30% chance of terminating early. If the ray was killed, we returned only direct lighting. Otherwise, we scaled our result to keep the estimator unbiased.</p>

        <p>Finally, we either added the indirect contribution to <code>L_out</code> (if <code>isAccumBounces</code> was true) or replaced <code>L_out</code> entirely. If there was no intersection, we returned black (no contribution).</p>
		
		<p>Here are some images rendered with global (direct and indirect) illumination using 1024 samples per pixel: </p>

		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
				<td style="text-align: center;">
					<img src="task4/global/4spheres.png" width="300px"/>
					<figcaption>Cornell Box spheres with 1024 samples/pixel and 16 samples/area_light.</figcaption>
				  </td>
				<td style="text-align: center;">
				  <img src="task4/global/4bunny.png" width="300px"/>
				  <figcaption>Bunny with 1024 samples/pixel and 16 samples/area_light.</figcaption>
				</td>
		</table>
		</div>

		<p>We can analyze the difference between direct and indirect lighting in the following two pictures of CBspheres, both at 1024 samples per pixel. 
			Both images use 16 samples per area light and <code>max_ray_depth=5</code>. All other parameters were default parameters.
		</p>

		<!-- insert two photos and also written analysis of the comparison -->
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
				<td style="text-align: center;">
					<img src="task4/CBsphere/spheres_direct.png" width="300px"/>
					<figcaption>Cornell Box spheres with only direct lighting {(zero bounce and one bounce)}</figcaption>
				  </td>
				<td style="text-align: center;">
				  <img src="task4/CBsphere/spheres_indirect.png" width="300px"/>
				  <figcaption>Cornell Box spheres with only indirect lighting {(second bounce to fifth bounce)}</figcaption>
				</td>
		</table>
		</div>

		<p>
			The image with only direct illumination shows the light coming from the light source itself and also the light all objects receive directly from the light source and reflect back to our eye in one bounce. 
			In the image with only indirect illumination, we can see the colors of the wall bleeding into the ball, and observe how light bounces off of the wall, onto the ball, and back into our eye. 
			We render until the fifth bounce, so all light bounces until then are included in this picture. 
			This picture, however, doesn’t include direct lighting which is why it is dimmer {(since these are all secondary, tertiary, etc. bounces we are looking at.)} 
		</p>

		<p>Here we render the unaccumulated m-th bounce of light for CBbunny.dae by setting <code>isAccumBounces=false</code> and manually setting the <code>-m</code> flag.
			All images are sampled at 1024 camera rays per pixel, with all other values set to their default values.
		</p>

		<!-- insert photos that we don't have yet 
		Explain in your write-up what you see for the 2nd and 3rd bounce of light, 
		and how it contributes to the quality of the rendered image compared to rasterization
		 -->

		<p>We also render the accumulated m-th bounces of light for CBbunny.dae below. All images are sampled at 1024 camera rays per pixel, with all other parameters using default values.</p>

		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
				<tr>
					<td style="text-align: center;">
						<img src="task4/CBbunny/isAccumBounceTrue/bunnyAccumM0.png" width="300px"/>
						<figcaption>0th bounce</figcaption>
					</td>
					<td style="text-align: center;">
					<img src="task4/CBbunny/isAccumBounceTrue/bunnyAccumM1.png" width="300px"/>
					<figcaption> 1st bounce</figcaption>
					</td>
					<td style="text-align: center;">
						<img src="task4/CBbunny/isAccumBounceTrue/bunnyAccumM2.png" width="300px"/>
						<figcaption> 2nd bounce</figcaption>
					</td>
				</tr>
				<tr>
					<td style="text-align: center;">
						<img src="task4/CBbunny/isAccumBounceTrue/bunnyAccumM3.png" width="300px"/>
						<figcaption>3rd bounce</figcaption>
					</td>
					<td style="text-align: center;">
					<img src="task4/CBbunny/isAccumBounceTrue/bunnyAccumM4.png" width="300px"/>
					<figcaption> 4th bounce</figcaption>
					</td>
					<td style="text-align: center;">
						<img src="task4/CBbunny/isAccumBounceTrue/bunnyAccumM5.png" width="300px"/>
						<figcaption> 5th bounce</figcaption>
					</td>
				</tr>
		</table>
		</div>

		<!-- need to compare rendered views of accumulated and unaccumulated bounces-->

		<p>The Russian Roulette rendering for CBbunny.dae is provided below for different values of <code>max_ray_depth</code> and with all images using 1024 samples per pixel. All other parameters are set to their default values.</p>
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
				<tr>
					<td style="text-align: center;">
						<img src="task4/CBbunny/russian roulette/bunnyRRM0.png" width="300px"/>
						<figcaption><code>max_ray_depth=0</code></figcaption>
					</td>
					<td style="text-align: center;">
					<img src="task4/CBbunny/russian roulette/bunnyRRM1.png" width="300px"/>
					<figcaption><code>max_ray_depth=1</code></figcaption>
					</td>
					<td style="text-align: center;">
						<img src="task4/CBbunny/russian roulette/bunnyRRM2.png" width="300px"/>
						<figcaption><code>max_ray_depth=2</code></figcaption>
					</td>
				</tr>
				<tr>
					<td style="text-align: center;">
						<img src="task4/CBbunny/russian roulette/bunnyRRM3.png" width="300px"/>
						<figcaption><code>max_ray_depth=3</code></figcaption>
					</td>
					<td style="text-align: center;">
					<img src="task4/CBbunny/russian roulette/bunnyRRM4.png" width="300px"/>
					<figcaption><code>max_ray_depth=4</code></figcaption>
					</td>
					<td style="text-align: center;">
						<img src="task4/CBbunny/russian roulette/bunnyRRM100.png" width="300px"/>
						<figcaption><code>max_ray_depth=100</code></figcaption>
					</td>
				</tr>
		</table>
		</div>

		<p>Below, we rendered CBspheres.dae with varying sample-per-pixel rates. All images use 4 light rays and <code>max_ray_depth=32</code>. All other parameters are set to their default values.</p>
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
				<tr>
					<td style="text-align: center;">
						<img src="task4/CBsphere/vary_sample/sphere1-4.png" width="300px"/>
						<figcaption><code>sample-per-pixel=1</code></figcaption>
					</td>
					<td style="text-align: center;">
						<img src="task4/CBsphere/vary_sample/sphere2-4.png" width="300px"/>
						<figcaption><code>sample-per-pixel=2</code></figcaption>
					</td>
				</tr>
				<tr>
					<td style="text-align: center;">
						<img src="task4/CBsphere/vary_sample/sphere4-4.png" width="300px"/>
						<figcaption><code>sample-per-pixel=4</code></figcaption>
					</td>
					<td style="text-align: center;">
						<img src="task4/CBsphere/vary_sample/sphere8-4.png" width="300px"/>
						<figcaption><code>sample-per-pixel=8</code></figcaption>
					</td>
				</tr>
				<tr>
					<td style="text-align: center;">
						<img src="task4/CBsphere/vary_sample/sphere16-4.png" width="300px"/>
						<figcaption><code>sample-per-pixel=16</code></figcaption>
					</td>
					<td style="text-align: center;">
						<img src="task4/CBsphere/vary_sample/sphere64-4.png" width="300px"/>
						<figcaption><code>sample-per-pixel=64</code></figcaption>
					</td>
				</tr>
				<tr>
					<td style="justify-self: center;">
						<img src="task4/CBsphere/vary_sample/sphere1024-4.png" width="300px"/>
						<figcaption><code>sample-per-pixel=1024</code></figcaption>
					</td>
				</tr>
		</table>
		</div>		

		<p>As the sample-per-pixel rate increases, the image becomes less noisy and converges to a smoother, more realistic image.
		</p>

		<h2>Part 5: Adaptive Sampling</h2>
		<p>
			In this part, we updated our function ray_trace_pixels to include adaptive sampling. 
			The main idea behind this is to raytrace more efficiently by checking whether certain pixels converge faster than the given sample rate and terminating their sampling early if they do converge early. 
		</p>

		<p>
			To measure their convergence, we use a variable <code>I = 1.96 * σ / √n</code>, which is derived from the 95% confidence interval formula. 
			We check <code>I &lt;= max_tolerance * μ</code>, where <code>max_tolerance = 0.05</code>; if this inequality is true, then we assume convergence and stop sampling the pixel. 
		</p>		

		<p>
			To actually implement this, we split the total number of samples into batches with samples_per_batches samples (a parameter we can change in the command line) and check the convergence of that batch. 
			We kept the code for sampling pixels from Part 1 to start off with, and built off of that. 
			We keep track of two variables:
		</p>

		<td style="justify-self: center;">
			<img src="task5/s1_s2_equation.png" width="300px"/>
		</td>

		<p>
			Here's the <li>x_k</li> represents the illuminance for each sample, which we acquire by calling <code>.illum()</code> on the radiance we sample.
			For each batch size, we compute <li>μ = s1 / n</li> and <li>σ^2 = 1 / (n - 1) * (s2 - s1^2 / n)</li>. 
			We then plug these values into <li>I = 1.96 * √σ^2 / n</li> and compare with <li>0.05 * μ</li>. If I is greater, then we terminate.
		</p>

		<p>
			Something we had to make sure to do is update the true number of samples we average over when calculating the average 
			- previously we were using <code>num_samples</code> to divide the sum of the radiance when we update <code>sampleBuffer</code> and <code>sampleCountBuffer</code>,
			but we updated this to use a tracker variable <code>n</code>, which represents the true number of samples.
		</p>

		<p>
			Here are some images with their rate pictures. Each image is sampled at 2048 samples per pixels, 1 sample per light, and uses <code>max_ray_depth=5</code>.
			All other parameters use default values.
		</p>

		<!--rendering images rn-->

		<!-- <h2>(Optional) Part 6: Extra Credit Opportunities</h2>
		Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum. -->

		</div>
	</body>
</html>