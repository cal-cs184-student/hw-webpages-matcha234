<html>
	<head>
		<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default'></script>
		<link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600&display=swap" rel="stylesheet">
		<style>
			h1 {
				text-align: center;
			}

			.container {
				margin: 0 auto;
				padding: 60px 20%;
			}

			figure {
				text-align: center;
			}

			img {
				display: inline-block;
			}

			body {
				font-family: 'Inter', sans-serif;
			}
		</style>
	</head>
	<body>
		<div class="container">
		<h1>CS184/284A Spring 2025 Homework 3 Write-Up</h1>
		<div style="text-align: center;">Advika Bhike and Malavikha Sudarshan</div>

		<br>

		Link to webpage: <a href="https://cal-cs184-student.github.io/hw-webpages-matcha234/hw3/index.html">cal-cs184-student.github.io/hw-webpages-matcha234/hw3/index.html</a>
		<br>

		Link to GitHub repository: <a href="https://github.com/cal-cs184-student/sp25-hw3-matchaaa">github.com/cal-cs184-student/sp25-hw3-matchaaa</a>
		
		<figure>
			<img src="cornell.png" alt="Cornell Boxes with Bunnies" style="width:70%"/>
		</figure>

		<!--
		We've already added one heading per part, to make your write-up as navigable when grading. Please fit your write-up within these sections!
		-->

		<h2>Overview</h2>
		Give a high-level overview of what you implemented in this homework. Think about what you've built as a whole. Share your thoughts on what interesting things you've learned from completing the homework.

		<h2>Part 1: Ray Generation and Scene Intersection</h2>

		<h3>Ray Generation and Primitive Intersections</h3>
		For ray generation, we first converted the given normalised image coordinates (x, y) into camera space. The virtual sensor was defined on the z = -1 plane, and we mapped 
		(x, y) to a point on the sensor using the field of view (FOV). The bottom-left and top-right 
		corners of the sensor were computed using:</p>
		
		<pre>
		left = -tan(hFov / 2), right = tan(hFov / 2)
		bottom = -tan(vFov / 2), top = tan(vFov / 2)
		</pre>
		
		<p>Using these values, we calculated the corresponding point on the sensor as:</p>
		
		<pre>
		sensor_x = left + (right - left) * x
		sensor_y = bottom + (top - bottom) * y
		</pre>
		<p>We then created a ray starting from the camera origin (0,0,0) and passing through this 
		calculated sensor point. The direction was initially set in camera space, then transformed 
		to world space using the camera-to-world transformation matrix <code>c2w</code> provided. Finally, we normalised 
		the direction vector and set the ray's near and far clipping distances to <code>nclip</code> and <code>fclip</code>.</p>
		
		<p>For triangle-ray intersections, we implemented a variation of the Möller-Trumbore algorithm. We first defined the edges of the triangle:</p>
		
		<pre>
		edge1 = v1 - v0
		edge2 = v2 - v0
		</pre>
		
		<p>Then, we calculated the determinant using the cross product:</p>
		
		<pre>
		h = cross(ray.dir, edge2)
		det = dot(edge1, h)
		</pre>
		
		<p>If the determinant was close to zero, the ray was parallel to the triangle, and so we ignored it.</p>
		
		<p>We then calculated barycentric coordinates to determine if the intersection point was inside the triangle:</p>
		
		<pre>
		f = 1.0 / det
		s = ray.origin - v0
		u = f * dot(s, h)
		</pre>
		
		<p>If u was not between 0 and 1, we discarded the intersection. We did something similar for v:</p>
		
		<pre>
		q = cross(s, edge1)
		v = f * dot(ray.dir, q)
		</pre>
		
		<p>If v was out of range or if u + v exceeded 1, then the intersection was outside the triangle.</p>
		
		<p>Finally, we computed t (the intersection distance along the ray):</p>
		
		<pre>
		t = f * dot(edge2, q)
		</pre>
		
		<p>If t was within the valid range (between <code>min_t</code> and <code>max_t</code> of the ray), we updated <code>max_t</code> to 
		this new intersection distance and stored the intersection details, including the interpolated normal 
		using barycentric coordinates.</p>

		<h3>Triangle Intersection</h3>
		<p>We first implemented <code>has_intersection</code>, a function that checked if a ray intersected the triangle without actually recording the details of the intersection. To implement this function, we did the following:</p>
		<ul>
			<li>First, we got the ray’s origin and direction.</li>
			<li>Then, we defined the triangle edges and calculated the normal vector using the cross product.</li>
			<li>We computed the intersection of the ray with the triangle’s plane and checked if it was within valid bounds.</li>
			<li>Using barycentric coordinates, we verified if the intersection point lay inside the triangle. If it did, we returned <code>true</code> and updated the ray’s <code>max_t</code>.</li>
		</ul>
	
		<p>The <code>intersect</code> function was similar to <code>has_intersection</code>, but it also recorded the intersection data when an intersection happened.</p>
		<ul>
			<li>First, we called <code>has_intersection</code> to check if the ray intersected the triangle at all. If not, we could immediately return <code>false</code>.</li>
			<li>We set up the ray’s origin, direction, and triangle’s edges again.</li>
			<li>Using the <code>max_t</code> from <code>has_intersection</code>, we computed the exact intersection point.</li>
			<li>We used barycentric coordinates to calculate the normal at the intersection point.</li>
			<li>Then, we stored the intersection details, including the intersection distance, normal, and material properties.</li>
		</ul>
		
		<p>Here are some images with normal shading:</p>
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
				  <img src="task1/CBempty.png" width="300px"/>
				  <figcaption>Cornell Box empty room</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="task1/CBspheres.png" width="300px"/>
				  <figcaption>Cornell Box spheres</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="task1/CBgems.png" width="300px"/>
				  <figcaption>Cornell Box gems</figcaption>
				</td>
			</tr>
			</table>
		</div>
		
		<h2>Part 2: Bounding Volume Hierarchy</h2>
		<p>For Part 2 of this homework, we implemented a BVH (Bounding Volume Hierarchy) construction algorithm to optimise ray-triangle intersection queries in our renderer, using a median primitive heuristic.</p>
	
		<p>We began by calculating the bounding box for the entire set of primitives within the range <code>start</code> to <code>end</code>. 
		We iterated through the primitives, expanding the bounding box <code>bbox</code> to surround all of them. 
		This bounding box represents the enclosing volume for the new BVH node. 
		
		<p>Next, we checked if the number of primitives in the newly created node was less than or equal to <code>max_leaf_size</code> to determine whether that node was a leaf node. 
			If it was a leaf node, we set the <code>start</code> and <code>end</code> iterators for the node and returned the node without instantiating the left and right pointers.
			If the node contained more than <code>max_leaf_size</code> primitives, we split the node into two child nodes, using the median primitive heuristic.
			To calculate this heuristic, we found the longest axis (between x, y or z) and sorted all the primitives based on their centroid position along that longest axis. 
			We calculated the midpoint of start and end to divide the now sorted vector of primitives, and recursively constructed our left and right nodes with different halves of the vector.
		</p>

		<p>We decided to split the primitives along the longest axis according to the median primitive because it helps create a fairer distribution between the two child nodes. 
			This makes it easier to calculate ray-primitive intersections because there's a more even distribution between nodes (as opposed to having less even-splitting heuristics, which could require
			something like searching an entire child node that has all primitives from the overall bounding box except one). 
			Essentially, splitting along the largest axis using the median primitive allows for better separation, which improves efficiency when checking for intersections.
		</p>

		<p>Here are some large .dae files with normal shading that can only be rendered feasibly with BVH acceleration:</p>
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
					<img src="task2/CBBunny.png" width="300px"/>
					<figcaption>Cornell Box bunny</figcaption>
				  </td>
				<td style="text-align: center;">
				  <img src="task2/CBdragon.png" width="300px"/>
				  <figcaption>Cornell Box dragon</figcaption>
				</td>
			</tr>
			<tr>
				<td style="text-align: center;">
				  <img src="task2/maxplanck.png" width="300px"/>
				  <figcaption>Max Planck</figcaption>
				</td>
				<td style="text-align: center;">
					<img src="task2/CBlucy.png" width="300px"/>
					<figcaption>Cornell Box Lucy</figcaption>
				  </td>
			</tr>
			</table>
		</div>

		<p>
			We compared the rendering times (BVH vs. no BVH) for a few images.
			The Cornell Box Bunny image renders in 0.0349s seconds with the BVH implementation, and in 66.7730s without.
			The Max Planck image took 0.0444s seconds with our BVH implementation and 98.1858s seconds without. 
			Cornell Box Lucy took 0.0451s seconds with our BVH implementation and 321.5604s seconds without.
			These are images with some highly complex geometries, and our BVH implementation helps render images that normally take multiple minutes in less than a second. 
		</p>
		
		<h2>Part 3: Direct Illumination</h2>

		<p>
			In this part, we implemented two types of direct lighting: one that uses uniform hemisphere sampling and another that uses importance sampling.
		</p>

		<p>
			First, we sampled our light direction and converted this direction into world coordinates. 
			We then traced a ray from the point <code>hit_p</code> (point of interest derived from the original ray of interest and original intersection of interest) in the direction of the sampled light source and checked whether this new ray would intersect with a light source. 
			If it did intersect a light source, we used the following reflection equation to calculate the outgoing light.
		</p>

		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
				<td style="text-align: center;">
					<img src="task3/reflection_equation.png" width="300px"/>
					<figcaption>Reflectance equation from lecture</figcaption>
				</td>
			</table>
		</div>
		
		<p>
			For each sampled light direction, we computed the inner term and kept summing these terms for num_samples number of times (this was variable and was set when we ran commands to render images).
			After summing, we divided by num_samples to take the average of this sum.
		</p>

		<p>
			Importance sampling, on the other hand, sampled all lights directly rather than uniformly on a hemisphere. 
			This type of sampling also used a Monte Carlo estimator. 
		</p>
		<p>
			We iterated through the light sources in the scene and sampled them num_samples number of times (which was variable and set when the rendering command was run). 
			If the light source was a point light source, we only sampled it once. 
			Otherwise, for each sample in num_samples, we sampled directions between the light source and <code>hit_p</code> and cast a ray in this direction. 
			If this new ray did not intersect any object, then we knew that it was a valid light-casting ray, and so we calculated the reflectance equation for it. 
			We took the Monte Carlo estimate of each light source and then added it to our global sum <code>L_out</code>. 
			It was important to note that we needed to compute a seperate Monte Carlo estimator for each light source 
			(specifically, take the average for each specific light source before adding to <code>L_out</code>) before adding it to the global sum because sometimes num_samples could be 1 (e.g. if the light was a point light) and other times it could be the given sample_rate. 
		</p>

		<p>Here are some images rendered by both types of direct lighting. The sample rate was varied to show how both sampling techniques progress with different sampling rates</p>
		<!-- insert photo -->

		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
					<img src="task3/uniform/CBbunny_H_16_8.png" width="300px"/>
					<figcaption>Cornell Box bunny with 16 samples/pixel and 8 samples/area_light using uniform hemisphere sampling</figcaption>
				  </td>
				<td style="text-align: center;">
				  <img src="task3/importance/CBbunny_16_8.png" width="300px"/>
				  <figcaption>Cornell Box bunny with 16 samples/pixel and 8 samples/area_light using importance sampling</figcaption>
				</td>
			</tr>
			<tr>
				<td style="text-align: center;">
				  <img src="task3/uniform/CBbunny_H_64_32.png" width="300px"/>
				  <figcaption>Cornell Box bunny with 64 samples/pixel and 32 samples/area_light using uniform hemisphere sampling</figcaption>
				</td>
				<td style="text-align: center;">
					<img src="task3/importance/bunny_64_32.png" width="300px"/>
					<figcaption>Cornell Box bunny with 64 samples/pixel and 32 samples/area_light using importance sampling</figcaption>
				  </td>
			</tr>

			<tr>
				<td style="text-align: center;">
				  <img src="task3/uniform/spheres_H_64_32.png" width="300px"/>
				  <figcaption>Cornell Box spheres with 64 samples/pixel and 32 samples/area_light using uniform hemisphere sampling</figcaption>
				</td>
				<td style="text-align: center;">
					<img src="task3/importance/spheres_64_32.png" width="300px"/>
					<figcaption>Cornell Box spheres with 64 samples/pixel and 32 samples/area_light using importance sampling</figcaption>
				  </td>
			</tr>
			</table>
		</div>

		<p>Here's how the Cornell Box bunny image varies for different light sample rates for direct light sampling. 
			The number of camera rays per pixels was held at a constant rate 1.</p>
		<!-- insert photo -->

		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			<tr>
				<td style="text-align: center;">
					<img src="task3/vary_sample_rate/CBBunny_1.png" width="300px"/>
					<figcaption>Light Sample Rate 1</figcaption>
				  </td>
			</tr>
			<tr>
				<td style="text-align: center;">
				  <img src="task3/vary_sample_rate/CBBunny_4.png" width="300px"/>
				  <figcaption>Light Sample Rate 4</figcaption>
				</td>
			</tr>
			<tr>
				<td style="text-align: center;">
				  <img src="task3/vary_sample_rate/CBBunny_16.png" width="300px"/>
				  <figcaption>Light Sample Rate 16</figcaption>
				</td>
			</tr>
			<tr>
				<td style="text-align: center;">
					<img src="task3/vary_sample_rate/CBBunny_64.png" width="300px"/>
					<figcaption>Light Sample Rate 64</figcaption>
				  </td>
			</tr>
			</table>
		</div>

		<p>
			The noise in the shadows decreased and became more blurred, more realistic and less disconnected as the light sample rate increased.
		</p>

		<p>
			In general, we can notice that the images using importance sampling tend to be less noisy. 
			This is because they directly sample from the light source, rather than assume the light source follows a uniform distribution. 
			Additionally, as both the camera sample rate and light sample rate increase, images rendered with importance sampling tend to converge faster to a higher quality, less-noisy image than uniform sampling does as demonstrated in the images above. 
			We can also see a clear progression of difference when only the light sample rate is changed when using importance light sampling. 
		</p>


		<h2>Part 4: Global Illumination</h2>

		<p>First, we built a local coordinate system based on the surface normal at the intersection point. We used <code>make_coord_space(o2w, isect.n)</code> to create an orthonormal basis and then took its transpose <code>w2o</code> to convert between world and local space. We calculated the intersection point <code>hit_p</code> and the outgoing direction <code>w_out</code> in local space using <code>w2o * (-r.d)</code>.</p>
        <p>We initialized <code>L_out</code> to black (0,0,0) and added direct lighting using <code>one_bounce_radiance</code>, but only if <code>isAccumBounces</code> was true. If we were only interested in indirect lighting, we skipped this step.</p>
        <p>Next, we used <code>isect.bsdf->sample_f(w_out, &w_i, &pdf)</code> to sample a new bounce direction, getting a random <code>w_i</code> along with its probability density function (pdf). If <code>pdf</code> was too small, we returned out of the function. We converted <code>w_i</code> from local to world space using <code>w_i_world = o2w * w_i</code> and created a new ray <code>new_ray</code> starting at <code>hit_p</code>. Then, we checked if the ray hit something using <code>bvh->intersect(new_ray, &new_isect)</code>. If there was an intersection, we recursively called <code>at_least_one_bounce_radiance</code> and scaled it by the BSDF, cosine term, and pdf.</p>
 
        <p>To avoid infinite recursion, we used Russian Roulette Termination with a 30% chance of terminating early. If the ray was killed, we returned only direct lighting. Otherwise, we scaled our result to keep the estimator unbiased.</p>

        <p>Finally, we either added the indirect contribution to <code>L_out</code> (if <code>isAccumBounces</code> was true) or replaced <code>L_out</code> entirely. If there was no intersection, we returned black (no contribution).</p>
		
		<p>Here are some images rendered with global (direct and indirect) illumination using 1024 samples per pixel: </p>

		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
				<td style="text-align: center;">
					<img src="task4/global/4spheres.png" width="300px"/>
					<figcaption>Cornell Box spheres with 1024 samples/pixel and 16 samples/area_light.</figcaption>
				  </td>
				<td style="text-align: center;">
				  <img src="task4/global/4bunny.png" width="300px"/>
				  <figcaption>Bunny with 1024 samples/pixel and 16 samples/area_light.</figcaption>
				</td>
		</table>
	</div>


		<h2>Part 5: Adaptive Sampling</h2>
		Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.

		<h2>(Optional) Part 6: Extra Credit Opportunities</h2>
		Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.
		
		<h2>Additional Notes (please remove)</h2>
		<ul>
			<li>You can also add code if you'd like as so: <code>code code code</code></li>
			<li>If you'd like to add math equations, 
				<ul>
					<li>You can write inline equations like so: \( a^2 + b^2 = c^2 \)</li>
					<li>You can write display equations like so: \[ a^2 + b^2 = c^2 \]</li>
				</ul>
			</li>
		</ul>
		</div>
	</body>
</html>